{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play Ground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncertainty: -1021.989577307477\n",
      "Surprise: -5.058893689053568\n"
     ]
    }
   ],
   "source": [
    "# Example in Chapter definition 4.10\n",
    "import math\n",
    "def calculate_uncertainty(probabilities, k):\n",
    "    uncertainty = k * sum(p * math.log2(p) for p in probabilities)\n",
    "    return uncertainty\n",
    "\n",
    "# Example usage\n",
    "probabilities = [0.7, 0.27, 0.03]\n",
    "k = 1000\n",
    "uncertainty = calculate_uncertainty(probabilities, k)\n",
    "print(\"Uncertainty:\", uncertainty)\n",
    "\n",
    "# surprise: individual uncertainty for event 3\n",
    "surprise = math.log2(probabilities[2])\n",
    "print(\"Surprise:\", surprise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncertainty: -14854.752972273343\n"
     ]
    }
   ],
   "source": [
    "# Law 5.1\n",
    "\n",
    "probabilities = [0.5, 0.3, 0.2]\n",
    "k = 10000\n",
    "uncertainty = calculate_uncertainty(probabilities, k)\n",
    "print(\"Uncertainty:\", uncertainty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "def counting_theorem(n, d):\n",
    "    if n == 0 or d == 0:\n",
    "        return 0\n",
    "    elif n == 1 or d == 1:\n",
    "        return 2\n",
    "    else:\n",
    "        return counting_theorem(n-1, d) + counting_theorem(n-1, d-1)\n",
    "    \n",
    "print(counting_theorem(4, 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 (a) (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "Information Capacity: 1.174 bits per parameter\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def generate_random_points_and_labels(n_points, n_features, c_classes):\n",
    "    # Generate random points with n_features\n",
    "    np.random.seed(2)\n",
    "    X = np.random.rand(n_points, n_features)\n",
    "    # Generate random labels with c equi-distributed classes\n",
    "    labels = np.random.choice(range(c_classes), n_points)\n",
    "    return X, labels\n",
    "\n",
    "def get_number_of_thresholds(data, labels):\n",
    "    table = [(np.sum(row), label) for row, label in zip(data, labels)]\n",
    "    sorted_table = sorted(table, key=lambda x: x[0])\n",
    "    thresholds = 0\n",
    "    previous_label = None\n",
    "    for data_sum, label in sorted_table:\n",
    "        if label != previous_label:\n",
    "            previous_label = label\n",
    "            thresholds += 1\n",
    "    return thresholds\n",
    "\n",
    "def train_nearest_neighbors(X_train, y_train):\n",
    "    # Train a nearest neighbors classifier\n",
    "    clf = KNeighborsClassifier(n_neighbors=1)  # Using 1-NN for simplicity\n",
    "    clf.fit(X_train, y_train)\n",
    "    return clf\n",
    "\n",
    "def get_number_of_instances_memorized(X_test, y_test, clf):\n",
    "    # Predict the labels for the test set\n",
    "    y_pred = clf.predict(X_test)\n",
    "    # For 1-NN, we'll consider the number of correctly classified instances as memorized instances\n",
    "    memorized_instances = np.sum(y_pred == y_test)\n",
    "    return memorized_instances\n",
    "\n",
    "def main(n_points=10000, n_features=2, c_classes=3):\n",
    "    # Step 1: Generate random points and labels\n",
    "    X, labels = generate_random_points_and_labels(n_points, n_features, c_classes)\n",
    "\n",
    "    # Step 2: Train the nearest neighbors classifier\n",
    "    clf = train_nearest_neighbors(X, labels)\n",
    "\n",
    "    # Step 3: Information Capacity\n",
    "    thresholds = get_number_of_thresholds(X, labels)\n",
    "    memorized_instances = get_number_of_instances_memorized(X, labels, clf)\n",
    "    print(memorized_instances)\n",
    "    info_capacity = memorized_instances / thresholds\n",
    "    print(f\"Information Capacity: {info_capacity:.3f} bits per parameter\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(n_points=128, n_features=10, c_classes=5)\n",
    "\n",
    "# c/c-1 4/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## generate data X (n_points, n_features) 000 001 010 011 100 101 110 111\n",
    "def generate_data(n_points, n_features):\n",
    "    data = np.zeros((n_points, n_features))\n",
    "    for i in range(n_points):\n",
    "        for j in range(n_features):\n",
    "            data[i, j] = i >> j & 1\n",
    "    return data\n",
    "generate_data(8, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d=2: n_full=4,           Avg. req. points for memorization n_avg=2.69,           n_full/n_avg=1.4883720930232558\n",
      "d=4: n_full=16,           Avg. req. points for memorization n_avg=8.94,           n_full/n_avg=1.7902097902097902\n",
      "d=6: n_full=64,           Avg. req. points for memorization n_avg=33.47,           n_full/n_avg=1.912231559290383\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def generate_random_points(n_points, n_features):\n",
    "    # X = np.random.rand(n_points, n_features)\n",
    "    X = np.random.randint(0, 2, (n_points, n_features))\n",
    "    return X\n",
    "\n",
    "def generate_data(n_points, n_features):\n",
    "    data = np.zeros((n_points, n_features))\n",
    "    for i in range(n_points):\n",
    "        for j in range(n_features):\n",
    "            data[i, j] = i >> j & 1\n",
    "    return data\n",
    "\n",
    "def generate_random_labels(n_points, c_classes):\n",
    "    labels = np.random.choice(range(c_classes), n_points)\n",
    "    return labels\n",
    "\n",
    "def generate_unique_random_labels(n_points, c_classes, n_labelings):\n",
    "    unique_labelings = set()\n",
    "    while len(unique_labelings) < n_labelings:\n",
    "        # Generate a random labeling\n",
    "        labels = tuple(np.random.choice(range(c_classes), n_points))\n",
    "        unique_labelings.add(labels)\n",
    "\n",
    "    # Convert each tuple back to a numpy array\n",
    "    unique_labelings = [np.array(labels) for labels in unique_labelings]\n",
    "    return unique_labelings\n",
    "\n",
    "def condensed_nearest_neighbor(X, y):\n",
    "    # Initialize S with one example from each class (optional)\n",
    "    # unique_labels = np.unique(y)\n",
    "    # S_indices = [np.where(y == label)[0][0] for label in unique_labels]\n",
    "    S_indices = [0]\n",
    "    changed = True\n",
    "    while changed:\n",
    "        changed = False\n",
    "        for i in range(len(X)):\n",
    "            if i in S_indices:\n",
    "                continue  # Skip if already in S\n",
    "            S = X[S_indices]\n",
    "            S_labels = y[S_indices]\n",
    "            knn = KNeighborsClassifier(n_neighbors=1)\n",
    "            knn.fit(S, S_labels)\n",
    "            pred = knn.predict([X[i]])[0]\n",
    "            if pred != y[i]:\n",
    "                S_indices.append(i)\n",
    "                changed = True\n",
    "    return len(S_indices)\n",
    "\n",
    "def main(d, num_function, c_classes):\n",
    "    n = 2**d\n",
    "    avg_mem_size = 0\n",
    "    X = generate_data(n, d)\n",
    "    # Y = generate_unique_random_labels(n, c_classes, num_function)\n",
    "    for i in range(num_function):\n",
    "        labels = generate_random_labels(n, c_classes)\n",
    "        req_points = condensed_nearest_neighbor(X, labels)\n",
    "        avg_mem_size += req_points\n",
    "    avg_mem_size /= num_function\n",
    "    print(f\"d={d}: n_full={2**d}, \\\n",
    "          Avg. req. points for memorization n_avg={avg_mem_size:.2f}, \\\n",
    "          n_full/n_avg={(2**d)/avg_mem_size}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(d=2, num_function=16, c_classes=2)\n",
    "    main(d=4, num_function=2**4, c_classes=2)\n",
    "    main(d=6, num_function=2**6, c_classes=2)\n",
    "    main(d=8, num_function=2**8, c_classes=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size: 1024, Condensed dataset size: 691\n",
      "Accuracy on original dataset: 1.00\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def condensed_nearest_neighbor(X, y):\n",
    "    # Initialize S with one example from each class (optional)\n",
    "    unique_labels = np.unique(y)\n",
    "    S_indices = [np.where(y == label)[0][0] for label in unique_labels]\n",
    "    \n",
    "    changed = True\n",
    "    while changed:\n",
    "        changed = False\n",
    "        for i in range(len(X)):\n",
    "            if i in S_indices:\n",
    "                continue  # Skip if already in S\n",
    "            S = X[S_indices]\n",
    "            S_labels = y[S_indices]\n",
    "            knn = KNeighborsClassifier(n_neighbors=1)\n",
    "            knn.fit(S, S_labels)\n",
    "            pred = knn.predict([X[i]])[0]\n",
    "            if pred != y[i]:\n",
    "                S_indices.append(i)\n",
    "                changed = True\n",
    "                \n",
    "    return X[S_indices], y[S_indices]\n",
    "\n",
    "# Example use\n",
    "# Generate random data\n",
    "d = 10\n",
    "n = 2 ** d\n",
    "X = np.random.rand(n, d)\n",
    "y = np.random.choice([0, 1], n)  # Ensure y has the same length as X\n",
    "\n",
    "X_condensed, y_condensed = condensed_nearest_neighbor(X, y)\n",
    "print(f\"Original dataset size: {len(X)}, Condensed dataset size: {len(X_condensed)}\")\n",
    "\n",
    "# Test the condensed dataset\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(X_condensed, y_condensed)\n",
    "print(f\"Accuracy on original dataset: {knn.score(X, y):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def generate_random_binary_dataset(n_samples=100, n_features=5):\n",
    "    X = np.random.randint(2, size=(n_samples, n_features))\n",
    "    y = np.random.randint(2, size=n_samples)\n",
    "    return X, y\n",
    "\n",
    "X, y = generate_random_binary_dataset()\n",
    "df = pd.DataFrame(X, columns=[f'Feature_{i}' for i in range(X.shape[1])])\n",
    "df['Target'] = y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m all_rules \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mzip\u001b[39m(row\u001b[38;5;241m.\u001b[39mindex[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], row\u001b[38;5;241m.\u001b[39mvalues[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])), row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTarget\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39miterrows()]\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Apply strategies\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m pruned_rules \u001b[38;5;241m=\u001b[39m \u001b[43mprune_redundant_rules\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_rules\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m combined_rules \u001b[38;5;241m=\u001b[39m combine_similar_rules(pruned_rules)\n\u001b[1;32m     43\u001b[0m ordered_rules \u001b[38;5;241m=\u001b[39m rule_ordering_and_early_stopping(combined_rules, df)\n",
      "Cell \u001b[0;32mIn[10], line 6\u001b[0m, in \u001b[0;36mprune_redundant_rules\u001b[0;34m(rules)\u001b[0m\n\u001b[1;32m      4\u001b[0m pruned_rules \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m rule \u001b[38;5;129;01min\u001b[39;00m rules:\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43many\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mall\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mother_rule\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrule\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mother_rule\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrules\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrule\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mother_rule\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m      7\u001b[0m         pruned_rules\u001b[38;5;241m.\u001b[39mappend(rule)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pruned_rules\n",
      "Cell \u001b[0;32mIn[10], line 6\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m pruned_rules \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m rule \u001b[38;5;129;01min\u001b[39;00m rules:\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28;43mall\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mother_rule\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrule\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m other_rule \u001b[38;5;129;01min\u001b[39;00m rules \u001b[38;5;28;01mif\u001b[39;00m rule \u001b[38;5;241m!=\u001b[39m other_rule):\n\u001b[1;32m      7\u001b[0m         pruned_rules\u001b[38;5;241m.\u001b[39mappend(rule)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pruned_rules\n",
      "Cell \u001b[0;32mIn[10], line 6\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m pruned_rules \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m rule \u001b[38;5;129;01min\u001b[39;00m rules:\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28mall\u001b[39m(\u001b[43mitem\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mother_rule\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m rule) \u001b[38;5;28;01mfor\u001b[39;00m other_rule \u001b[38;5;129;01min\u001b[39;00m rules \u001b[38;5;28;01mif\u001b[39;00m rule \u001b[38;5;241m!=\u001b[39m other_rule):\n\u001b[1;32m      7\u001b[0m         pruned_rules\u001b[38;5;241m.\u001b[39mappend(rule)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pruned_rules\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "def prune_redundant_rules(rules):\n",
    "    pruned_rules = []\n",
    "    for rule in rules:\n",
    "        if not any(all(item in other_rule for item in rule) for other_rule in rules if rule != other_rule):\n",
    "            pruned_rules.append(rule)\n",
    "    return pruned_rules\n",
    "\n",
    "def combine_similar_rules(rules):\n",
    "    combined_rules = []\n",
    "    for rule_a, rule_b in combinations(rules, 2):\n",
    "        diff = [item for item in rule_a if item not in rule_b] + [item for item in rule_b if item not in rule_a]\n",
    "        if len(diff) == 2 and diff[0][:-2] == diff[1][:-2]:  # Only one condition differs\n",
    "            new_rule = tuple(set(rule_a).union(set(rule_b)) - set(diff))\n",
    "            if new_rule not in combined_rules:\n",
    "                combined_rules.append(new_rule)\n",
    "    return combined_rules + [rule for rule in rules if rule not in combined_rules]\n",
    "\n",
    "def rule_ordering_and_early_stopping(rules, dataset):\n",
    "    # Sort rules by coverage (number of samples it applies to)\n",
    "    rules_coverage = {rule: sum(all(row[feature] == value for feature, value in rule[:-1]) for _, row in dataset.iterrows()) for rule in rules}\n",
    "    sorted_rules = sorted(rules, key=lambda rule: -rules_coverage[rule])\n",
    "    # Early stopping is implemented during rule evaluation, not here\n",
    "    return sorted_rules\n",
    "\n",
    "def evaluate_rules(rules, dataset):\n",
    "    correct_predictions = 0\n",
    "    for _, row in dataset.iterrows():\n",
    "        for rule in rules:\n",
    "            if all(row[feature] == value for feature, value in rule[:-1]):\n",
    "                correct_predictions += rule[-1] == row['Target']\n",
    "                break  # Early stopping\n",
    "    accuracy = correct_predictions / len(dataset)\n",
    "    return accuracy\n",
    "\n",
    "# Generate all possible if-then clauses (rules) from the dataset\n",
    "all_rules = [(tuple(zip(row.index[:-1], row.values[:-1])), row['Target']) for _, row in df.iterrows()]\n",
    "\n",
    "# Apply strategies\n",
    "pruned_rules = prune_redundant_rules(all_rules)\n",
    "combined_rules = combine_similar_rules(pruned_rules)\n",
    "ordered_rules = rule_ordering_and_early_stopping(combined_rules, df)\n",
    "\n",
    "# Evaluate the rules\n",
    "accuracy = evaluate_rules(ordered_rules, df)\n",
    "print(f\"Number of rules after minimization: {len(ordered_rules)}\")\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.59"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import zlib\n",
    "import numpy as np\n",
    "\n",
    "# Generate a long random string\n",
    "random_string = ''.join(np.random.choice(['A', 'C', 'G', 'T'], size=100))\n",
    "\n",
    "# Convert the string to bytes\n",
    "random_string_bytes = random_string.encode('utf-8')\n",
    "\n",
    "# Compress the string using zlib\n",
    "compressed_string = zlib.compress(random_string_bytes)\n",
    "\n",
    "# Calculate the compression ratio\n",
    "compression_ratio = len(compressed_string) / len(random_string_bytes)\n",
    "\n",
    "compression_ratio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expected compression ratio for a random string, especially one composed of a small set of characters (like 'A', 'C', 'G', 'T' in the case of our DNA sequence example), can vary significantly depending on several factors, including the length of the string, the distribution of characters, and the compression algorithm used. However, random data or sequences with high entropy (where each character is equally likely and independent of the others) are generally hard to compress effectively because the essence of lossless compression is to find and exploit patterns, redundancy, or predictability in the data.\n",
    "\n",
    "For the example given, where the string is generated from an equal distribution of four characters, we might expect a lower compression ratio than highly redundant or patterned data. This is because, even though there are repeating characters, the randomness and uniform distribution reduce the predictability and hence the compressibility of the sequence. \n",
    "\n",
    "Compression algorithms like zlib implement a combination of strategies (such as dictionary-based compression in DEFLATE, which zlib uses) to reduce data size. These strategies work best when the data has noticeable patterns or redundancy. In the case of a truly random sequence, the compression algorithm might struggle to find useful patterns to exploit, leading to less significant compression. \n",
    "\n",
    "The observed compression ratio of approximately 0.3195 suggests that zlib was able to find some patterns or redundancies to exploit, possibly due to the limited character set, even though the sequence was intended to be random. This ratio might not be as good as for data with more obvious patterns (like repeated sequences or large blocks of identical characters) but is still notable given the randomness of the input.\n",
    "\n",
    "It's important to note that for truly random data, the theoretical best compression you could expect (without losing information) is very minimal, due to the Shannon entropy of the data being high. In practical terms, this means that the more random and less predictable the data, the closer the compression ratio will approach 1 (no compression), after accounting for the overhead of the compression algorithm itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40.30296890880646"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def memorize(data, labels):\n",
    "    \"\"\"\n",
    "    Function to calculate the minimum encoding cost (mec) for given data and labels.\n",
    "    \n",
    "    Parameters:\n",
    "    data (np.array): An array of n d-dimensional vectors.\n",
    "    labels (np.array): A column of 0s or 1s with length n.\n",
    "    \n",
    "    Returns:\n",
    "    float: The minimum encoding cost (mec).\n",
    "    \"\"\"\n",
    "    # Calculate the sum for each row and pair it with its label\n",
    "    table = [(np.sum(row), label) for row, label in zip(data, labels)]\n",
    "    \n",
    "    # Sort the table based on the sum (column 0)\n",
    "    sorted_table = sorted(table, key=lambda x: x[0])\n",
    "    \n",
    "    # Initialize variables\n",
    "    thresholds = 0\n",
    "    class_ = 0\n",
    "    \n",
    "    # Determine the number of threshold changes\n",
    "    for row in sorted_table:\n",
    "        if row[1] != class_:\n",
    "            class_ = row[1]\n",
    "            thresholds += 1\n",
    "    # Calculate the minimum number of thresholds\n",
    "    min_threshs = np.log2(thresholds + 1)\n",
    "    \n",
    "    # Calculate minimum encoding cost\n",
    "    mec = (min_threshs * (data.shape[1] + 1)) + (min_threshs + 1)\n",
    "    \n",
    "    return mec\n",
    "\n",
    "# Example usage\n",
    "n = 100\n",
    "d = 5\n",
    "np.random.seed(42)\n",
    "data = np.random.rand(n, d)  # n d-dimensional vectors\n",
    "labels = np.random.randint(2, size=n)  # n binary labels\n",
    "mec = memorize(data, labels)\n",
    "mec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40.30296890880646"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modification for multi-class labels\n",
    "\n",
    "def memorize_multiclass(data, labels):\n",
    "    \"\"\"\n",
    "    Function to calculate the minimum encoding cost (mec) for given data and labels for multi-class classification.\n",
    "    \n",
    "    Parameters:\n",
    "    data (np.array): An array of n d-dimensional vectors.\n",
    "    labels (np.array): A column of labels with length n, for multi-class classification.\n",
    "    \n",
    "    Returns:\n",
    "    float: The minimum encoding cost (mec).\n",
    "    \"\"\"\n",
    "    # Calculate the sum for each row and pair it with its label\n",
    "    table = [(np.sum(row), label) for row, label in zip(data, labels)]\n",
    "    \n",
    "    # Sort the table based on the sum (column 0)\n",
    "    sorted_table = sorted(table, key=lambda x: x[0])\n",
    "    \n",
    "    # Initialize variables\n",
    "    thresholds = 0\n",
    "    previous_label = None\n",
    "    \n",
    "    # Determine the number of threshold changes\n",
    "    for data_sum, label in sorted_table:\n",
    "        if label != previous_label:\n",
    "            previous_label = label\n",
    "            thresholds += 1\n",
    "    # Calculate the minimum number of thresholds\n",
    "    min_threshs = np.log2(thresholds + 1)\n",
    "    \n",
    "    # Calculate minimum encoding cost\n",
    "    d = data.shape[1]\n",
    "    mec = (min_threshs * (d + 1)) + (min_threshs + 1)\n",
    "    \n",
    "    return mec\n",
    "\n",
    "# Example usage with multi-class labels\n",
    "n = 100\n",
    "d = 5\n",
    "np.random.seed(42)\n",
    "data = np.random.rand(n, d)  # n d-dimensional vectors\n",
    "labels = np.random.randint(2, size=n)  # n labels for multi-class classification\n",
    "\n",
    "mec_multiclass = memorize_multiclass(data, labels)\n",
    "mec_multiclass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:00<00:00, 43409094.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 104698093.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:00<00:00, 30168781.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 14195625.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Epoch 1, Loss: 0.23937382253963174\n",
      "Epoch 2, Loss: 0.11206717976728386\n",
      "Epoch 3, Loss: 0.08702044609089149\n",
      "Epoch 4, Loss: 0.07455138549499556\n",
      "Epoch 5, Loss: 0.0684255073962759\n",
      "Epoch 6, Loss: 0.05826709370372315\n",
      "Epoch 7, Loss: 0.05079960703640295\n",
      "Epoch 8, Loss: 0.046488662213417214\n",
      "Epoch 9, Loss: 0.043231522153807565\n",
      "Epoch 10, Loss: 0.039125274848761354\n",
      "Accuracy: 99.24%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "# Define a flexible CNN model\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self, conv_layers, linear_layers, dropout_rate=0.5):\n",
    "        super(CustomCNN, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        # Add convolutional layers\n",
    "        for in_channels, out_channels, kernel_size, stride, padding in conv_layers:\n",
    "            self.layers.append(nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding))\n",
    "            self.layers.append(nn.BatchNorm2d(out_channels))\n",
    "            self.layers.append(nn.ReLU(inplace=True))\n",
    "            self.layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "        self.layers.append(nn.Flatten())\n",
    "\n",
    "        # Add linear layers\n",
    "        for in_features, out_features in linear_layers[:-1]:\n",
    "            self.layers.append(nn.Linear(in_features, out_features))\n",
    "            self.layers.append(nn.ReLU(inplace=True))\n",
    "            self.layers.append(nn.Dropout(dropout_rate))\n",
    "\n",
    "        # Add the final linear layer without dropout\n",
    "        in_features, out_features = linear_layers[-1]\n",
    "        self.layers.append(nn.Linear(in_features, out_features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "# Hyperparameters and model configuration - easily modifiable\n",
    "conv_layers = [\n",
    "    (1, 32, 3, 1, 1), # in_channels, out_channels, kernel_size, stride, padding\n",
    "    (32, 64, 3, 1, 1),\n",
    "]\n",
    "\n",
    "linear_layers = [\n",
    "    (7*7*64, 128), # in_features, out_features\n",
    "    (128, 10),\n",
    "]\n",
    "\n",
    "# Data preparation\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_dataset = MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = CustomCNN(conv_layers, linear_layers, dropout_rate=0.5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f'Accuracy: {100 * correct / total}%')\n",
    "\n",
    "# Run training and evaluation\n",
    "train_model(model, train_loader, criterion, optimizer, num_epochs=10)\n",
    "evaluate_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters in the model: 421834\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = count_parameters(model)\n",
    "print(f\"Total trainable parameters in the model: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "421834"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_parameters(conv_layers, linear_layers):\n",
    "    total_params = 0\n",
    "    # Calculate parameters for convolutional layers\n",
    "    for in_channels, out_channels, kernel_size, stride, padding in conv_layers:\n",
    "        conv_params = out_channels * (in_channels * kernel_size * kernel_size + 1)  # +1 for bias\n",
    "        bn_params = 2 * out_channels  # BatchNorm has running_mean and running_var\n",
    "        total_params += conv_params + bn_params\n",
    "    # Calculate parameters for linear layers\n",
    "    for in_features, out_features in linear_layers:\n",
    "        linear_params = out_features * (in_features + 1)  # +1 for bias\n",
    "        total_params += linear_params\n",
    "    return total_params\n",
    "\n",
    "calculate_parameters(conv_layers, linear_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of neurons in the model: 234\n"
     ]
    }
   ],
   "source": [
    "def count_neurons(model):\n",
    "    neuron_count = 0\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            # For linear layers, the number of out_features is the number of neurons\n",
    "            neuron_count += layer.out_features\n",
    "        elif isinstance(layer, nn.Conv2d):\n",
    "            # For Conv2d layers, use the number of out_channels as an approximation of neurons\n",
    "            neuron_count += layer.out_channels\n",
    "        elif isinstance(layer, nn.MaxPool2d):\n",
    "            # Optional: Include pooling layers if considering them as having neurons\n",
    "            # Note: This would be an approximation and depends on input size\n",
    "            pass\n",
    "    return neuron_count\n",
    "\n",
    "# Example usage with the CustomCNN model\n",
    "# model = CustomCNN(conv_layers, linear_layers)\n",
    "print(f\"The number of neurons in the model: {count_neurons(model)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape: (60000, 784), labels shape: (60000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def dataset_to_ndarray(dataset):\n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    for image, label in dataset:\n",
    "        # Convert the image to a NumPy array and flatten it\n",
    "        np_image = image.numpy().flatten()\n",
    "        data.append(np_image)\n",
    "        \n",
    "        # Append the label\n",
    "        labels.append(label)\n",
    "    \n",
    "    # Convert lists to NumPy arrays\n",
    "    data = np.array(data)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    return data, labels\n",
    "\n",
    "# Convert train_dataset to data and labels NumPy arrays\n",
    "data, labels = dataset_to_ndarray(train_dataset)\n",
    "\n",
    "print(f\"data shape: {data.shape}, labels shape: {labels.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory-Equivalent Capacity (MEC): 12362.058469281907\n"
     ]
    }
   ],
   "source": [
    "mec = memorize(data, labels)\n",
    "print(f\"Memory-Equivalent Capacity (MEC): {mec}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs285",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
